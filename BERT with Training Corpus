pip install transformers torch

from huggingface_hub import login

login(token="your token")

pip install torch transformers datasets

pip install sacremoses

import pandas as pd
from datasets import Dataset, DatasetDict
from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer

# Load your data
data = pd.read_csv('/content/corpus2.csv.csv')
df = pd.DataFrame(data)
dataset = Dataset.from_pandas(df)

# Tokenize data
model_name = "shhossain/opus-mt-en-to-bn"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

def tokenize_function(examples):
    model_inputs = tokenizer(examples["English"], max_length=128, truncation=True, padding="max_length")
    labels = tokenizer(text_target=examples["Bengali"], max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Split the dataset into training and testing datasets
train_test_split = tokenized_datasets.train_test_split(test_size=0.2)
dataset_dict = DatasetDict({
    'train': train_test_split['train'],
    'test': train_test_split['test']
})

# Set up training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    report_to="none"
)

# Initialize the trainer using processing_class instead of tokenizer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset_dict["train"],
    eval_dataset=dataset_dict["test"],
    processing_class=tokenizer  # Use this if you are on a newer version
)

# Start training
trainer.train()

from transformers import MarianMTModel, MarianTokenizer
# Assuming model and tokenizer are already loaded
input_sentence = "Input your sentence"
# Tokenize the input
inputs = tokenizer(input_sentence, return_tensors="pt", padding=True, truncation=True)

# Generate translation with explicit generation parameters
translation_tokens = model.generate(**inputs, max_length=512, num_beams=4, bad_words_ids=[[tokenizer.pad_token_id]])

# Decode the generated tokens to a string
translation = tokenizer.decode(translation_tokens[0], skip_special_tokens=True)
print("Translated:", translation)
